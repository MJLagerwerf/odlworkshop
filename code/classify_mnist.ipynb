{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits.\n",
    "\n",
    "The particular dataset we will use is the MNIST (Modified National Institute of Standards and Technology)\n",
    "The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a machine to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Set the random seed to enable reproducible code\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and utilities\n",
    "\n",
    "We now need to get the data we will use, which in this case is the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of digits 70000 hand-written digits, of which 60000 are used for training and 10000 for testing.\n",
    "\n",
    "In addition to this, we create a utility `evaluate(...)` that we will use to evaluate how good the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(mnist.test.images[0].reshape(28,-1),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the 10000 mnist test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = mnist.test.next_batch(10000)\n",
    "test_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "test_labels = batch[1]\n",
    "\n",
    "def evaluate(result_tensor, data_placeholder):\n",
    "    \"\"\"Evaluate a reconstruction method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_tensor : `tf.Tensor`, shape (None,)\n",
    "        The tensorflow tensor containing the result of the classification.\n",
    "    data_placeholder : `tf.Tensor`, shape (None, 28, 28, 1) or (None, 784)\n",
    "        The tensorflow tensor containing the input to the classification operator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : float\n",
    "        Mean squared error of the reconstruction.\n",
    "    \"\"\"\n",
    "    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n",
    "    result = result_tensor.eval(\n",
    "        feed_dict={data_placeholder: feed_images})\n",
    "\n",
    "    return np.mean(result == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders. Placeholders are needed in tensorflow since tensorflow is a lazy language,\n",
    "and hence we first define the computational graph with placeholders as input, and later we evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('placeholders'):\n",
    "    images = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    true_labels = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We start with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), perhaps the most well known and widely applied classification method.\n",
    "\n",
    "The first problem we need to solve is that the values we try to regress against are discrete (e.g. [0, 1, 2, ..., 9]) which does not work very well with continuous optimization. To solve this we convert the values to a one-hot encoding, embedding the values into $\\mathbb{R}^{10}$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toh = tf.one_hot([0, 1, 2], depth=3)\n",
    "toh.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as a probabilistic encoding, i.e. we can estimate that a number is 10% 1 and 90% 2. For our training data, we have 100% certanity for each digit. \n",
    "\n",
    "The estimator used for logistic regression is\n",
    "\n",
    "$$\n",
    "p_x(\\text{label=$i$}) = \\frac{\\exp(\\langle w_i, x \\rangle + b_i)}{\\sum_{j=0}^9 \\exp(\\langle w_j, x \\rangle + b_j)}\n",
    "$$\n",
    "\n",
    "Here, $p_x(\\text{label=$i$})$ is the probability of an image $x$ belonging to a category $i$, $w_i \\in \\mathbb{R}^{28 \\times 28}$ and $b_i \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is a comparison between the probability distribution $p_x$ and the deterministic probability distribution $q_x$.\n",
    "We use the *cross entropy* for this. The loss function for each image is:\n",
    "\\\\[\n",
    "-\\sum_{i=0}^9 q_x(\\text{label=$i$}) \\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is the mean value of the cross entropy (implicitly assuming that the images are uniformly distributed):\n",
    "\\\\[\n",
    "L(p) := -\\frac{1}{N}\\sum_{x=1}^N\\sum_{i=0}^9 q_x(\\text{label=$i$})\\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an elementary implementation in `TensorFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('elementary'):\n",
    "    X = tf.placeholder(shape=(None, 784), dtype=tf.float32, name=\"X\")\n",
    "    weights = tf.Variable(tf.random_normal((784, 10)), name=\"weights\")\n",
    "    bias = tf.Variable(tf.zeros((10)), name=\"bias\")\n",
    "    lin = tf.matmul(X, weights)\n",
    "    lin_ = lin + bias\n",
    "    elin_ = tf.exp(lin_)\n",
    "    Z = tf.reduce_sum(tf.exp(lin_), axis=1, keep_dims=True)\n",
    "    prob = elin_ / Z\n",
    "    log_prob = tf.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_loss\"):\n",
    "    labels = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    determ = tf.one_hot(labels, depth=10)\n",
    "    loss = -tf.reduce_mean(determ*log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_training\"):    \n",
    "    learning_rate = .1\n",
    "    batch_size = 2**7\n",
    "\n",
    "    variables = [weights, bias]\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    update_ops = [var.assign(var - learning_rate*grad) \n",
    "                  for var, grad in zip(variables, gradients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feed_dict={labels:mnist.train.labels[:batch_size], X:mnist.train.images[:batch_size]}\n",
    "for i in range(100000):\n",
    "    images_, labels_ = mnist.train.next_batch(batch_size)\n",
    "    session.run(update_ops, \n",
    "                feed_dict={labels:labels_, X:images_})\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(log_prob, axis=1), X)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorFlow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.flatten(images)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('{} Average correct: {}'.format(\n",
    "                i, evaluate(pred, images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "The first \"deep\" neural networks were [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron), in these we have a function of the following form\n",
    "\n",
    "$$\n",
    "\\rho(W_3\\rho(W_2\\rho(W_1 x + b_1) + b_2) + b_3)\n",
    "$$\n",
    "\n",
    "Where $W_i$ are matrices and $b_i$ vectors. Note that the logistic regression can be cast into this form (how?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.flatten(images)\n",
    "    x = tf.contrib.layers.fully_connected(x, 128)  # the default activation function is ReLU\n",
    "    x = tf.contrib.layers.fully_connected(x, 32)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('{} Average correct: {}'.format(\n",
    "                i, evaluate(pred, images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Convolutional neural networks are a corner-stone of the deep learning revolution. Here instead of using traditionall fully-connected layers which connect each point with all other points, we use spatial convolutions instead. By doing this, we get a translation invariant operator that acts locally. In order to get non-local behaviour we stack several of these on top of each other.\n",
    "\n",
    "The following code is a very simplified convolutional neural network for digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('convolutional_network'):\n",
    "    x = tf.contrib.layers.conv2d(images, num_outputs=32, kernel_size=3, stride=2)\n",
    "    x = tf.contrib.layers.conv2d(x, num_outputs=32, kernel_size=3, stride=2)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    \n",
    "    x = tf.contrib.layers.fully_connected(x, 128)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('{} Average correct: {}'.format(\n",
    "                i, evaluate(pred, images)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
