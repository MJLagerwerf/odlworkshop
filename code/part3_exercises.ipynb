{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Part 3\n",
    "\n",
    "These small exercises are intended to build on the knowledge that you acquired in the [Learned Reconstruction](part3_learned_reconstruction) notebook. The exercises do not depend on each other. Feel free to pick those you like and do them in any order.\n",
    "\n",
    "We do not indend for all attendants to solve all of these during the course, but recommend that you at least read them.\n",
    "\n",
    "### Related exercises\n",
    "\n",
    "All of the exercises from Part 2 can also be applied to inverse problems. Try them out here as well!\n",
    "\n",
    "### General note\n",
    "\n",
    "If you have trouble understanding the explanations, or if there is an error, please [let us know](https://github.com/odlgroup/odl/issues). These examples are meant to be understandable pretty much without prior knowledge, and we appreciate any feedback on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1: Primal-Dual Reconstruction\n",
    "\n",
    "In the example code, we implemented a so called Learned Gradient scheme. This type of scheme only operates on the image, and not on the data. Another type of reconstruction scheme is [Learned Primal-Dual Reconstruction](https://arxiv.org/abs/1707.06474), in these schemes we iterate on both image and data. A very simple version would be given by\n",
    "\n",
    "$$\n",
    "x_0 = \\mathcal{T}^\\dagger(y)\n",
    "\\\\\n",
    "h_0 = y\n",
    "\\\\\n",
    "h_{i+1} = \\Gamma_{\\theta_i^d}(h_i, \\mathcal{T}(x_i), g) \\\\\n",
    "x_{i+1} = \\Lambda_{\\theta_i^p}(x_i, \\mathcal{T}^*(h_i)) \\\\\n",
    "\\mathcal{T}_\\theta^\\dagger = x_I\n",
    "$$\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Implement a learned primal-dual scheme. How do the results compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2: Memory in learned iterative schemes\n",
    "\n",
    "In the learned gradient scheme in the examples, and in the learned primal-dual scheme above, the only connection between the iterates is $x_i \\in \\mathcal{X}$ (and $h_i \\in \\mathcal{Y}$).\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* What happens if we instead use $x_0 = [\\mathcal{T}^\\dagger(y), ..., \\mathcal{T}^\\dagger(y)]$. Do the results improve? What about doing the same for y?\n",
    "* In so called [DenseNets](https://arxiv.org/abs/1608.06993), we retain information from the previous iterates. For example, we could use\n",
    "$$\n",
    "h_{i+1} = \\Gamma_{\\theta_i^d}(h_i, h_{i-1}, ..., h_0, \\mathcal{T}(x_i), g) \\\\\n",
    "$$\n",
    "How does this change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
