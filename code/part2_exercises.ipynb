{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "These small exercises are intended to build on the knowledge that you acquired in the [classify MNIST](part2_classification) notebook. The exercises do not depend on each other. Feel free to pick those you like and do them in any order.\n",
    "\n",
    "We do not indend for all attendants to solve all of these during the course, but recommend that you at least read them.\n",
    "\n",
    "### General note\n",
    "\n",
    "If you have trouble understanding the explanations, or if there is an error, please [let us know](https://github.com/odlgroup/odl/issues). These examples are meant to be understandable pretty much without prior knowledge, and we appreciate any feedback on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Residual networks\n",
    "\n",
    "Residual networks were introduced in *Deep Residual Learning for Image Recognition*, He et. al. 2015 [arXiv](https://arxiv.org/abs/1512.03385). In residual networks instead of using the Multi-Layer Perceptron (MLP) structure\n",
    "\n",
    "$$\n",
    "x_{n+1} = \\rho(W_nx_n + b_n)\n",
    "$$\n",
    "\n",
    "We use a residual structure\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n + W_n^{(2)}\\rho(W_n^{(1)}x_n + b_n)\n",
    "$$\n",
    "\n",
    "In tensorflow, such a residual unit could be written as\n",
    "\n",
    "```\n",
    "tmp = tf.contrib.layers.fully_connected(x, n1)\n",
    "x = x + tf.contrib.layers.fully_connected(tmp, n2, \n",
    "                                          activation_fn=None)\n",
    "```\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Implement a residual version of the MLP. Does this allow trainig a deeper network?\n",
    "* Implement a residual convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Data augmentation\n",
    "\n",
    "In all of the earlier examples, we have assume that the training data is finite, but there are several invariances that we expect from our data, for example an image of a certain digit is also an image of the same digit if the image is translated slightly or if some slight noise is added to the image.\n",
    "\n",
    "In data augmentation, we extend our dataset by adding these to the training data, and there is strong support for this in e.g. tensorflow.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Use [`tf.contrib.image.translate`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/image/translate) to implement data augmentation. How does the results change? Does this allow you to train larger networks?\n",
    "* Use the function [`tf.random_normal`](https://www.tensorflow.org/api_docs/python/tf/random_normal) in order to implement data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Training\n",
    "\n",
    "In the earlier examples we either used plain batch stochastic gradient descent (manually) or the ADAM optimizer [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) with a fixed learning rate (step size), but these choices are not set in stone, and the training speed and result of a neural network may in fact depend rather strongly on the training used!\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Train the networks using other optimizers such as [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer) or [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). How does the convergence rate change? Does the end-result change? Why?\n",
    "* The default learning rate is typically $\\approx 10^{-4}$. Try different learning rates by supplying the `learning_rate` parameter to the optimizer, e.g. `tf.train.AdamOptimizer(learning_rate=1e-1)`. How does this change the training speed? What about the final result?\n",
    "* By creating a placeholder and using it as a learning rate, the learning rate can be varied:\n",
    "```\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "...\n",
    "session.run(optimizer, feed_dict={images: train_images, \n",
    "                                    true_labels: train_labels,\n",
    "                                    learning_rate: ???})\n",
    "```\n",
    "What happens if you start with a high learning rate and then decrease it over time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
