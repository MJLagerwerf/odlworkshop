{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits.\n",
    "\n",
    "The particular dataset we will use is the MNIST (Modified National Institute of Standards and Technology)\n",
    "The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a machine to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "In these exercies we'll use the software library [tensorflow](https://www.tensorflow.org/), which is a state of the art library for numerical computations in python developed by Google. The package supports computation on both the CPU and GPU, and also supports automatic differentiation (more on that later).\n",
    "\n",
    "Writing code in tensorflow is quite similar to classical python, except for one thing: tensorflow uses *deffered execution*, this means that instead of executing the code in one step (called eager execution), the computational graph is first defined, and it is then fed data upon which the computation is performed. Users from compiled languages will recongnize this execution method.\n",
    "\n",
    "The execution flow typically goes like this:\n",
    "\n",
    "* Define placeholders for the input\n",
    "* Define what computations should be performed\n",
    "* Feed the placeholders data\n",
    "\n",
    "This computational model has several upsides that are sadly not evident in simple samples like these, including parallelization, packaging and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Set the random seed to enable reproducible code\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and utilities\n",
    "\n",
    "We now need to get the data we will use, which in this case is the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of digits 70000 hand-written digits, of which 60000 are used for training and 10000 for testing.\n",
    "\n",
    "In addition to this, we create a utility `evaluate(...)` that we will use to evaluate how good the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f133a292e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[0].reshape(28,-1),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the 10000 mnist test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = mnist.test.next_batch(10000)\n",
    "test_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "test_labels = batch[1]\n",
    "\n",
    "def evaluate(result_tensor, data_placeholder):\n",
    "    \"\"\"Evaluate a reconstruction method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_tensor : `tf.Tensor`, shape (None,)\n",
    "        The tensorflow tensor containing the result of the classification.\n",
    "    data_placeholder : `tf.Tensor`, shape (None, 28, 28, 1) or (None, 784)\n",
    "        The tensorflow tensor containing the input to the classification operator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : float\n",
    "        Mean squared error of the reconstruction.\n",
    "    \"\"\"\n",
    "    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n",
    "    result = result_tensor.eval(\n",
    "        feed_dict={data_placeholder: feed_images})\n",
    "\n",
    "    return np.mean(result == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We start with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), perhaps the most well known and widely applied classification method.\n",
    "\n",
    "The first problem we need to solve is that the values we try to regress against are discrete (e.g. [0, 1, 2, ..., 9]) which does not work very well with continuous optimization. To solve this we convert the values to a one-hot encoding, embedding the values into $\\mathbb{R}^{10}$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toh = tf.one_hot([0, 1, 2], depth=3)\n",
    "toh.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as a probabilistic encoding, i.e. we can estimate that a number is 10% 1 and 90% 2. For our training data, we have 100% certanity for each digit. \n",
    "\n",
    "The estimator used for logistic regression is\n",
    "\n",
    "$$\n",
    "p_x(\\text{label=$i$}) = \\frac{\\exp(\\langle w_i, x \\rangle + b_i)}{\\sum_{j=0}^9 \\exp(\\langle w_j, x \\rangle + b_j)}\n",
    "$$\n",
    "\n",
    "Here, $p_x(\\text{label=$i$})$ is the probability of an image $x$ belonging to a category $i$, $w_i \\in \\mathbb{R}^{28 \\times 28}$ and $b_i \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is a comparison between the probability distribution $p_x$ and the deterministic probability distribution $q_x$.\n",
    "We use the *cross entropy* for this. The loss function for each image is:\n",
    "\\\\[\n",
    "-\\sum_{i=0}^9 q_x(\\text{label=$i$}) \\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is the mean value of the cross entropy (implicitly assuming that the images are uniformly distributed):\n",
    "\\\\[\n",
    "L(p) := -\\frac{1}{N}\\sum_{x=1}^N\\sum_{i=0}^9 q_x(\\text{label=$i$})\\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an elementary implementation in `TensorFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('elementary_network'):\n",
    "    # Create a placeholder for our input data (no computation is done here)\n",
    "    X = tf.placeholder(shape=(None, 784), dtype=tf.float32, name=\"X\")\n",
    "    \n",
    "    # Create the parameters (weight, bias) of the model\n",
    "    weights = tf.Variable(tf.random_normal((784, 10)), name=\"weights\")\n",
    "    bias = tf.Variable(tf.zeros((10)), name=\"bias\")\n",
    "    \n",
    "    # Compute the probabilities (this is all lazy, no computations are actually performed)\n",
    "    lin = tf.matmul(X, weights) + bias\n",
    "    elin = tf.exp(lin)\n",
    "    Z = tf.reduce_sum(elin, axis=1, keep_dims=True)\n",
    "    prob = elin / Z\n",
    "    log_prob = tf.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function which measures how good our parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_loss\"):\n",
    "    labels = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    determ = tf.one_hot(labels, depth=10)\n",
    "    loss = -tf.reduce_mean(determ*log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Define the gradient descent update step, i.e.\n",
    "\n",
    "$$w_i \\leftarrow w_i - \\omega \\nabla_{w_i} L(w, b)$$\n",
    "\n",
    "where $\\omega$ is the *learning rate*, or step size.\n",
    "\n",
    "Note that in machine learning, we typically use *stochastic* gradient descent (SGD). In these methods we don't use all of the data to compute the gradient, only a small subset called a mini-batch. Here we use 128 images in each training step.\n",
    "\n",
    "Further, while for this case computing the gradient would be quite simple, once we move to harder and mroe complicated models doing so would be basically impossible to do by hand. To work around this, all major deep learning frameworks implement [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). This may sound fancy, but automatic differentiation is simply the chain rule for the derivative. Tensorflow implements it using the `tf.gradients` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_training\"):\n",
    "    learning_rate = .1\n",
    "    batch_size = 128\n",
    "\n",
    "    variables = [weights, bias]\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    update_ops = [var.assign(var - learning_rate*grad) \n",
    "                  for var, grad in zip(variables, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the code above was lazy, nothing has actually happened. Before we start we need to initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network by feeding data from the training set and occationally evalute the performance on our test set, this is the first point we actually start doing computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4%, 40.8%, 58.0%, 66.2%, 70.9%, 73.8%, 75.7%, 77.3%, 78.5%, 79.5%, 80.2%, 80.7%, 81.3%, 81.9%, 82.5%, 82.8%, 83.2%, 83.5%, 83.8%, 84.1%, 84.3%, 84.5%, 84.8%, 85.0%, 85.1%, 85.4%, 85.5%, 85.5%, 85.8%, 86.0%, 86.1%, 86.2%, 86.3%, 86.4%, 86.6%, 86.7%, 86.8%, 86.8%, 86.9%, 86.9%, 87.1%, 87.1%, 87.2%, 87.4%, 87.4%, 87.5%, 87.6%, 87.6%, 87.7%, 87.8%, 87.8%, 87.8%, 87.9%, 88.0%, 88.0%, 88.1%, 88.3%, 88.3%, 88.3%, 88.3%, 88.3%, 88.4%, 88.5%, 88.5%, 88.5%, 88.5%, 88.6%, 88.6%, 88.6%, 88.6%, 88.6%, 88.7%, 88.7%, 88.8%, 88.9%, 88.8%, 88.8%, 88.9%, 88.9%, 88.9%, 89.0%, 89.0%, 89.0%, 89.1%, 89.1%, 89.2%, 89.2%, 89.2%, 89.2%, 89.3%, 89.3%, 89.3%, 89.4%, 89.4%, 89.4%, 89.3%, 89.5%, 89.5%, 89.6%, 89.6%, "
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    images_, labels_ = mnist.train.next_batch(batch_size)\n",
    "    session.run(update_ops, \n",
    "                feed_dict={labels:labels_, X:images_})\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(log_prob, axis=1), X)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow libraries\n",
    "\n",
    "While the above code solves our problem, it involved several small and perhaps obscure steps. Once we start moving to more complicated neural networks the code would become very repetetive.\n",
    "\n",
    "Since all of the steps are standardized, we can (and should) instead use built in tensorflow functions, this example does that, and all following examples will do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('placeholders'):\n",
    "    images = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    true_labels = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "The \"network\" can be computed using the `tf.contrib.layers.fully_connected` function, which computes\n",
    "\n",
    "$$\\rho(Ax + b)$$\n",
    "\n",
    "where $\\rho$ is the activation function, $A$ the weights and $b$ the bias. Note that here we never explicitly construct these, they are hidden inside tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.flatten(images)\n",
    "    logits = tf.contrib.layers.fully_connected(x, \n",
    "                                               num_outputs=10,\n",
    "                                               activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimization\n",
    "\n",
    "The loss function defined above should be done using the `tf.nn.softmax_cross_entropy_with_logits` function, which not only is easier to use, it is also more numerically stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "Training the network looks about the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2%, 91.3%, 92.1%, 92.5%, 92.5%, 92.7%, 92.8%, 92.8%, 92.9%, 92.7%, "
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), images)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "The first \"deep\" neural networks were [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron), in these we have a function of the following form\n",
    "\n",
    "$$\n",
    "\\rho(W_3\\rho(W_2\\rho(W_1 x + b_1) + b_2) + b_3)\n",
    "$$\n",
    "\n",
    "Where $W_i$ are matrices and $b_i$ vectors. Note that the logistic regression can be cast into this form (how?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0%, 96.4%, 97.4%, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0afd9d4aaca0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     session.run(optimizer, feed_dict={images: train_images, \n\u001b[1;32m---> 25\u001b[1;33m                                       true_labels: train_labels})\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.flatten(images)\n",
    "    x = tf.contrib.layers.fully_connected(x, num_outputs=128)  # the default activation function is ReLU\n",
    "    x = tf.contrib.layers.fully_connected(x, num_outputs=32)\n",
    "    logits = tf.contrib.layers.fully_connected(x, \n",
    "                                               num_outputs=10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), images)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Convolutional neural networks are a corner-stone of the deep learning revolution. Here instead of using traditionall fully-connected layers which connect each point with all other points, we use spatial convolutions instead. By doing this, we get a translation invariant operator that acts locally. In order to get non-local behaviour we stack several of these on top of each other.\n",
    "\n",
    "The following code is a very simplified convolutional neural network for digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('convolutional_network'):\n",
    "    x = tf.contrib.layers.conv2d(images, \n",
    "                                 num_outputs=32, # Number of \"channels\", e.g. duplicates of the image\n",
    "                                 kernel_size=3,  # size of the convolution kernel\n",
    "                                 stride=2)       # Use strides (jumps) to decrease the image size in each step\n",
    "    x = tf.contrib.layers.conv2d(x, num_outputs=32, kernel_size=3, stride=2)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    \n",
    "    x = tf.contrib.layers.fully_connected(x, 128)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(true_labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    train_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "    train_labels = batch[1]\n",
    "\n",
    "    session.run(optimizer, feed_dict={images: train_images, \n",
    "                                      true_labels: train_labels})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), images)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* Software libraries like tensorflow are great\n",
    "* Neural networks can be used to classify numbers\n",
    "* Deeper neural networks seem to work quite well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
